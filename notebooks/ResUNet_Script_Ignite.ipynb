{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a2565a",
   "metadata": {},
   "source": [
    "## Import Modules and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf4ac67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:16.739888Z",
     "iopub.status.busy": "2022-08-12T20:27:16.739574Z",
     "iopub.status.idle": "2022-08-12T20:27:21.078046Z",
     "shell.execute_reply": "2022-08-12T20:27:21.077278Z",
     "shell.execute_reply.started": "2022-08-12T20:27:16.739827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import random\n",
    "\n",
    "from processing_functions import *\n",
    "\n",
    "# import deep learning libraries\n",
    "from torchvision import transforms, utils\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Metric, Loss\n",
    "import pprint\n",
    "\n",
    "\n",
    "# from monai.losses import DiceLoss\n",
    "from monai.losses import DiceLoss, FocalLoss, DiceFocalLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import BasicUNet, UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")\n",
    "\n",
    "# import processing libraries\n",
    "from patchify import unpatchify\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cacbae03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.079548Z",
     "iopub.status.busy": "2022-08-12T20:27:21.079323Z",
     "iopub.status.idle": "2022-08-12T20:27:21.083009Z",
     "shell.execute_reply": "2022-08-12T20:27:21.082300Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.079520Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12b8a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.086922Z",
     "iopub.status.busy": "2022-08-12T20:27:21.086704Z",
     "iopub.status.idle": "2022-08-12T20:27:21.138078Z",
     "shell.execute_reply": "2022-08-12T20:27:21.137349Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.086896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # cloud server\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# macbook\n",
    "# # use_mps = torch.has_mps\n",
    "# use_mps = False\n",
    "# device = torch.device(\"mps\" if use_mps else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fa7efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.139232Z",
     "iopub.status.busy": "2022-08-12T20:27:21.138987Z",
     "iopub.status.idle": "2022-08-12T20:27:21.145823Z",
     "shell.execute_reply": "2022-08-12T20:27:21.145133Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.139205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0bc5b",
   "metadata": {},
   "source": [
    "## Read TIF images into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2205da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.147038Z",
     "iopub.status.busy": "2022-08-12T20:27:21.146794Z",
     "iopub.status.idle": "2022-08-12T20:27:21.181765Z",
     "shell.execute_reply": "2022-08-12T20:27:21.181096Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.147011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import tifffile\n",
    "import glob\n",
    "\n",
    "# Cloud Server\n",
    "raw_path = \"/home/jovyan/workspace/Images/Raw/*.tif\"\n",
    "mask_path = \"/home/jovyan/workspace/Images/Mask/*.tif\"\n",
    "\n",
    "# # Jason's Desktop\n",
    "# raw_path = \"I:\\My Drive\\Raw\\*.tif\"\n",
    "# mask_path = \"I:\\My Drive\\Mask\\*.tif\"\n",
    "\n",
    "# # Jason's Macbook\n",
    "# raw_path = \"/Users/jasonfung/haaslabdataimages@gmail.com - Google Drive/My Drive/Raw/*.tif\"\n",
    "# mask_path = \"/Users/jasonfung/haaslabdataimages@gmail.com - Google Drive/My Drive/Mask/*.tif\"\n",
    "\n",
    "raw_filename_list = glob.glob(raw_path) \n",
    "mask_filename_list = glob.glob(mask_path)\n",
    "\n",
    "# Pre Shuffle\n",
    "raw_filename_list.sort()\n",
    "mask_filename_list.sort()\n",
    "\n",
    "# Shuffle the filename list\n",
    "from sklearn.utils import shuffle\n",
    "raw_filename_list, mask_filename_list = shuffle(raw_filename_list, mask_filename_list, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644105bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.182929Z",
     "iopub.status.busy": "2022-08-12T20:27:21.182710Z",
     "iopub.status.idle": "2022-08-12T20:27:21.187324Z",
     "shell.execute_reply": "2022-08-12T20:27:21.186669Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.182902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/workspace/Images/Raw/000_B_181107_A_N1B2_4a61736f.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_D_180906_A_N1C1_53696168.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180613_N4_4a61736f.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_ML_20180614_N5_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_ML_20180614_N3_53696168.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_GFPC_20180615_N1_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180622_N2_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20190604_A_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_D_180907_A_N1B3_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/060_B_181031_A_N1B3_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_GFP_181027_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180622_N1_53696168.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20190604_B_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_ML_20180614_N1_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_D_190306_A_N1A2_4d696368.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180621_A_52616a61.tif']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09472f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.188512Z",
     "iopub.status.busy": "2022-08-12T20:27:21.188297Z",
     "iopub.status.idle": "2022-08-12T20:27:21.193933Z",
     "shell.execute_reply": "2022-08-12T20:27:21.193288Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.188486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_filename_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3a6ea",
   "metadata": {},
   "source": [
    "## Processing Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e3d2e",
   "metadata": {},
   "source": [
    "## Patching and Reconstruction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ceebbd",
   "metadata": {},
   "source": [
    "Input of an image is (# z stacks, # x pixels, # y pixels). Then it is split into sub volumes of size (z patch size, x patch size, y patch size) where each subvolume is (voxelized), meaning it has a coordinate relative to the original image. This ends up becoming a (z location, x location, y location, z patch size, x patch size, y patch size).\n",
    "\n",
    "If the image has a depth that cannot be evenly split by 2^n depth patch size where n is an integer, the function patch_images() will return an \"upper half\" and a \"lower half\" of the image. For example, given an image size of (77,512,512) and a z patch size of 16, x patch size of 32, and y patch of 32, the image cannot be evenly split into 16ths, since 77 % 16 = 4 remainder 13. Therefore, starting from the top of the stack to slice 64: the upper half becomes (64, 512, 512). Contrastly, starting from the bottom of the stack at slice 77 to slice 61, the lower half becomes (16, 512, 512). The two split volumes will become merged in the end.\n",
    "\n",
    "To prepare for training, the coordinated subvolumes are reshaped into \"batch form\". From the previous example with the upper half (64, 512, 512) split and coordinated into a 6D array (4, 16, 16, 16, 32, 32) -> (4 * 16 * 16, 16, 32, 32).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4863e",
   "metadata": {},
   "source": [
    "## Split Training and Testing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04052ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.196388Z",
     "iopub.status.busy": "2022-08-12T20:27:21.196156Z",
     "iopub.status.idle": "2022-08-12T20:27:21.205651Z",
     "shell.execute_reply": "2022-08-12T20:27:21.204920Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.196346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# define patching parameters\n",
    "lateral_steps = 64\n",
    "axial_steps = 16\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "split_size = 0.8\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "#                                       new_shape(new_xy = (600,960)),\n",
    "                                      MinMaxScalerVectorized(),\n",
    "                                      patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = False)])\n",
    "\n",
    "# define transforms for labeled masks\n",
    "label_transforms = transforms.Compose([\n",
    "#                                        new_shape(new_xy = (600,960)),\n",
    "                                       process_masks(int_class = 3),\n",
    "                                       patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = True)])\n",
    "\n",
    "\n",
    "raw_training_list, mask_training_list = raw_filename_list[:int(split_size*len(raw_filename_list))], mask_filename_list[:int(split_size*len(mask_filename_list))]\n",
    "raw_testing_list, mask_testing_list = raw_filename_list[int(split_size*len(raw_filename_list)):], mask_filename_list[int(split_size*len(mask_filename_list)):]\n",
    "print(len(raw_training_list))\n",
    "\n",
    "training_data = MyImageDataset(raw_training_list,\n",
    "                               mask_training_list,\n",
    "                               transform = patch_transform,\n",
    "                               label_transform = label_transforms,\n",
    "                               device = device,\n",
    "                               img_order = dim_order,\n",
    "                               mask_order = dim_order,\n",
    "                               num_classes = 4,\n",
    "                               train=True)\n",
    "\n",
    "testing_data = MyImageDataset(raw_testing_list,\n",
    "                              mask_testing_list,\n",
    "                              transform = patch_transform,\n",
    "                              label_transform = label_transforms,\n",
    "                              device = device,\n",
    "                              img_order = dim_order,\n",
    "                              mask_order = dim_order,\n",
    "                              num_classes = 4,\n",
    "                              train=False)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "training_dataloader = DataLoader(training_data, batch_size = 1, shuffle = False)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b795bcc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:21.207076Z",
     "iopub.status.busy": "2022-08-12T20:27:21.206862Z",
     "iopub.status.idle": "2022-08-12T20:27:25.880937Z",
     "shell.execute_reply": "2022-08-12T20:27:25.880128Z",
     "shell.execute_reply.started": "2022-08-12T20:27:21.207050Z"
    }
   },
   "outputs": [],
   "source": [
    "upper, upper_shape, lower, lower_shape, full_mask, mask_upper, mask_lower = next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4af20c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.882142Z",
     "iopub.status.busy": "2022-08-12T20:27:25.881919Z",
     "iopub.status.idle": "2022-08-12T20:27:25.886850Z",
     "shell.execute_reply": "2022-08-12T20:27:25.886201Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.882113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553aead",
   "metadata": {},
   "source": [
    "# Define Model and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8e6c4",
   "metadata": {},
   "source": [
    "### Model: ResUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ebbc7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.887960Z",
     "iopub.status.busy": "2022-08-12T20:27:25.887750Z",
     "iopub.status.idle": "2022-08-12T20:27:25.959060Z",
     "shell.execute_reply": "2022-08-12T20:27:25.958343Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.887934Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up loss and optimizer\n",
    "max_epochs = 150\n",
    "dropout = 0.2\n",
    "learning_rate = 5e-5\n",
    "# decay = 1e-5\n",
    "input_chnl = 1\n",
    "output_chnl = 4\n",
    "\n",
    "model = UNet(spatial_dims=3, \n",
    "             in_channels = input_chnl,\n",
    "             out_channels = output_chnl,\n",
    "             channels = (16, 32, 64, 128, 256),\n",
    "             strides=(2, 2, 2, 2),\n",
    "             num_res_units=2,\n",
    "             norm = \"batch\",\n",
    "             dropout = dropout)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d980327a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.960239Z",
     "iopub.status.busy": "2022-08-12T20:27:25.960019Z",
     "iopub.status.idle": "2022-08-12T20:27:25.965204Z",
     "shell.execute_reply": "2022-08-12T20:27:25.964488Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.960212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4810977\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cda337",
   "metadata": {},
   "source": [
    "## Loss, Metric, Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa5c3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.966324Z",
     "iopub.status.busy": "2022-08-12T20:27:25.966108Z",
     "iopub.status.idle": "2022-08-12T20:27:25.973921Z",
     "shell.execute_reply": "2022-08-12T20:27:25.973228Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.966298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    }
   ],
   "source": [
    "# loss_function = FocalLoss()\n",
    "loss_function = DiceCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_epochs/30, verbose = True)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5, patience = 10, threshold=1e-5, threshold_mode= 'abs', verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "discretize = Compose([Activations(softmax = True), \n",
    "                      AsDiscrete(logit_thresh=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37e77c",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45e452dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.975086Z",
     "iopub.status.busy": "2022-08-12T20:27:25.974860Z",
     "iopub.status.idle": "2022-08-12T20:27:25.979573Z",
     "shell.execute_reply": "2022-08-12T20:27:25.978818Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.975060Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Augmentation Function parameters\n",
    "# degree = (25, 5, 5)\n",
    "# translate = (10,10,10)\n",
    "# transform_rotate = torchio.RandomAffine(degrees=degree, \n",
    "#                                         translation=translate, \n",
    "#                                         image_interpolation=\"bspline\")\n",
    "\n",
    "# transform_flip = torchio.RandomFlip(axes=('ap',))\n",
    "# all_transform = torchio.Compose([transform_rotate,\n",
    "#                                  transform_flip])\n",
    "\n",
    "# Augmentation Function parameters using resnet parameters\n",
    "degree = (25, 0, 0)\n",
    "# translate = (10,10,10)\n",
    "transform_rotate = torchio.RandomAffine(degrees=degree, \n",
    "#                                         translation=translate, \n",
    "                                        image_interpolation=\"bspline\")\n",
    "transform_flip = torchio.RandomFlip(axes=('ap',))\n",
    "all_transform = torchio.Compose([transform_rotate,\n",
    "                                 transform_flip])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb08b4",
   "metadata": {},
   "source": [
    "## Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366ad7e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:25.980935Z",
     "iopub.status.busy": "2022-08-12T20:27:25.980715Z",
     "iopub.status.idle": "2022-08-12T20:27:26.004970Z",
     "shell.execute_reply": "2022-08-12T20:27:26.004269Z",
     "shell.execute_reply.started": "2022-08-12T20:27:25.980908Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(engine, batch):\n",
    "\n",
    "    batch_size = 64\n",
    "    lateral_steps = 64\n",
    "    axial_steps = 16\n",
    "    patch_size = (axial_steps, lateral_steps, lateral_steps)    \n",
    "    augment = True\n",
    "    shuffle = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count_loss = 0\n",
    "    # Instantiate the dice sum for each class\n",
    "    \n",
    "    upper_img, upper_shape, lower_img, lower_shape, full_mask, upper_mask, lower_mask = batch\n",
    "    # upper_img: dict() of {\"index 1\": upper_raw_tensor_1, \"index 2\": upper_raw_tensor_2, ..., \"index n\": upper_raw_tensor_n} raw_tensor = FloatTensor(Z,Y,X)\n",
    "    # lower_img: dict() of {\"index 1\": lower_raw_tensor_1, \"index 2\": lower_raw_tensor_2\", ..., \"index n\": lower_raw_tensor_n} raw_tensor = FloatTensor(Z,Y,X)\n",
    "    # upper_shape: tuple() representing shape of the upper volume (z,y,x) Note: this is used for reconstruction\n",
    "    # lower_shape: tuple() representing shape of the lower volume (z,y,x) Note: this is used for reconstruction\n",
    "    # full_mask: torch.FloatTensor of size (B,C,Z,Y,X) B = batch, C = Class Channel \n",
    "    # upper_mask: dict() of {\"index 1\": upper_mask_tensor_1, \"index 2\": upper_mask_tensor_2, ..., \"index n\": upper_mask_tensor_n} mask_tensor = FloatTensor(C,Z,Y,X)\n",
    "    # lower_mask: dict() of {\"index 1\": upper_mask_tensor_1, \"index 2\": upper_mask_tensor_2, ..., \"index n\": lower_mask_tensor_n} mask_tensor = FloatTensor(C,Z,Y,X)\n",
    "    # Empty list to place subvolumes in\n",
    "    \n",
    "    tmp_upper_dict = {}\n",
    "    tmp_lower_dict = {}    \n",
    "    \n",
    "    if shuffle == True:\n",
    "        # shuffle the batches\n",
    "        upper_key_list = list(range(len(upper_img)))\n",
    "        random.shuffle(upper_key_list)\n",
    "        \n",
    "        # check if lower img exists, otherwise perform shuffling\n",
    "        if lower_img == None:\n",
    "            pass\n",
    "        else:\n",
    "            lower_key_list = list(range(len(lower_img)))\n",
    "            random.shuffle(upper_key_list)\n",
    "    else:\n",
    "        upper_key_list = list(range(len(upper_img)))\n",
    "        lower_key_list = list(range(len(lower_img)))\n",
    "    \n",
    "    \n",
    "    # Only train on evenly split images\n",
    "    if lower_img == None:\n",
    "        num_subvolumes = len(upper_img)\n",
    "        for bindex in trange(0, num_subvolumes, batch_size):\n",
    "            if bindex + batch_size > num_subvolumes:\n",
    "                # if the bindex surpasses the number of number of sub volumes\n",
    "                batch_keys = upper_key_list[bindex:num_subvolumes]\n",
    "            else:\n",
    "                batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "            \n",
    "            sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "            sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(sub_imgs)\n",
    "            probabilities = torch.softmax(output, 1)\n",
    "            \n",
    "            # discretize probability values \n",
    "            prediction = torch.argmax(probabilities, 1)\n",
    "            tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "            \n",
    "            # calculate the loss for the current batch, save the loss per epoch to calculate the average running loss\n",
    "            current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += current_loss.item()\n",
    "        \n",
    "        # lower list does not exist\n",
    "        tmp_lower_list = None\n",
    "            \n",
    "    # train on both \n",
    "    else:\n",
    "        num_upper_subvolumes = len(upper_img)\n",
    "        if augment:\n",
    "            # Extract index of non-zero subvolumes\n",
    "            upper_indexes = get_index_nonempty_cubes(upper_mask)\n",
    "            \n",
    "            # Augment on non-zero subvolumes based on their location in the volume (by index)\n",
    "            for bindex in range(0, len(upper_indexes), batch_size):\n",
    "                # for augmentation\n",
    "                if bindex + batch_size > len(upper_indexes):\n",
    "                    upper_batch = upper_indexes[bindex:len(upper_indexes)]\n",
    "                else:\n",
    "                    upper_batch = upper_indexes[bindex:bindex+batch_size]\n",
    "                    \n",
    "                sub_imgs, sub_masks = augmentation(all_transform, upper_img, upper_mask, upper_batch)\n",
    "                sub_imgs, sub_masks = sub_imgs.to(device), sub_masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(output, 1)\n",
    "                prediction = torch.argmax(probabilities, 1)\n",
    "                \n",
    "                current_loss = loss_function(probabilities, sub_masks)\n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "        \n",
    "        for bindex in range(0, num_upper_subvolumes, batch_size):\n",
    "            if bindex + batch_size > num_upper_subvolumes:\n",
    "                # if the bindex surpasses the number of number of sub volumes\n",
    "                batch_keys = upper_key_list[bindex:num_upper_subvolumes]\n",
    "            else:\n",
    "                batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "            \n",
    "            sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "            sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(sub_imgs) # predict the batches\n",
    "            probabilities = torch.softmax(output, 1) \n",
    "            prediction = torch.argmax(probabilities,1)\n",
    "            \n",
    "            # update the upper img dictionary\n",
    "            tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "            \n",
    "            current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "            \n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += current_loss.item()\n",
    "            count_loss += 1\n",
    "        \n",
    "        num_lower_subvolumes = len(lower_img)\n",
    "        if augment:\n",
    "            \n",
    "            lower_indexes = get_index_nonempty_cubes(lower_mask)\n",
    "            \n",
    "            for bindex in range(0, len(lower_indexes), batch_size):\n",
    "                # for augmentation\n",
    "                if bindex + batch_size > len(lower_indexes):\n",
    "                    lower_batch = lower_indexes[bindex:len(lower_indexes)]\n",
    "                else:\n",
    "                    lower_batch = lower_indexes[bindex:bindex+batch_size]\n",
    "                    \n",
    "                sub_imgs, sub_masks = augmentation(all_transform, lower_img, lower_mask, lower_batch)\n",
    "                sub_imgs, sub_masks = sub_imgs.to(device), sub_masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(output, 1)\n",
    "                prediction = torch.argmax(probabilities, 1)\n",
    "                \n",
    "                current_loss = loss_function(probabilities, sub_masks)\n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "                \n",
    "        for bindex in range(0, num_lower_subvolumes, batch_size):\n",
    "            if bindex + batch_size > num_lower_subvolumes:\n",
    "                # if the bindex surpasses the number of number of sub volumes\n",
    "                batch_keys = lower_key_list[bindex:num_lower_subvolumes]\n",
    "            else:\n",
    "                batch_keys = lower_key_list[bindex:bindex+batch_size]\n",
    "            \n",
    "            sub_imgs = torch.squeeze(torch.stack([lower_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "            sub_masks = torch.squeeze(torch.stack([lower_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(sub_imgs)\n",
    "            probabilities = torch.softmax(output, 1)\n",
    "            prediction = torch.argmax(probabilities,1)\n",
    "            \n",
    "            # update the lower dictionary\n",
    "            tmp_lower_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "            current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += current_loss.item()\n",
    "            count_loss += 1\n",
    "\n",
    "        orig_shape = full_mask.shape[1:-1]\n",
    "        reconstructed_mask_order = (3,0,1,2)\n",
    "\n",
    "\n",
    "        upper_values = torch.stack([tmp_upper_dict[key] for key in list(range(len(tmp_upper_dict)))])\n",
    "        lower_values = torch.stack([tmp_lower_dict[key] for key in list(range(len(tmp_lower_dict)))])\n",
    "\n",
    "\n",
    "        reconstructed = reconstruct_training_masks(upper_values, lower_values, upper_shape, \n",
    "                                                   lower_shape, patch_size, orig_shape) # returns (z,y,x)\n",
    "        reconstructed = to_categorical_torch(reconstructed, num_classes = 4) # returns (z,y,x,c)\n",
    "        reconstructed = reconstructed.type(torch.int16)\n",
    "        reconstructed = torch.permute(reconstructed, reconstructed_mask_order)\n",
    "        reconstructed = torch.unsqueeze(reconstructed, 0) # make reconstructed image into (Batch,c,z,y,x)\n",
    "\n",
    "#         full_mask = full_mask.type(torch.int16)\n",
    "        gt_mask = torch.permute(full_mask, dim_order).cpu() # roll axis of grount truth mask into (batch,c,z,y,x)\n",
    "        \n",
    "    return {\"batch_loss\":running_loss/count_loss, \"y_pred\":reconstructed, \"y\":gt_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "788090b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.006166Z",
     "iopub.status.busy": "2022-08-12T20:27:26.005951Z",
     "iopub.status.idle": "2022-08-12T20:27:26.021598Z",
     "shell.execute_reply": "2022-08-12T20:27:26.020899Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.006140Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(engine, batch):\n",
    "    \n",
    "    batch_size = 64\n",
    "    axial_steps = 16\n",
    "    patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "    shuffle = False\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        count_loss = 0\n",
    "\n",
    "        upper_img, upper_shape, lower_img, lower_shape, full_mask, upper_mask, lower_mask = batch\n",
    "        # Empty list to place subvolumes in\n",
    "        tmp_upper_dict = {}\n",
    "        tmp_lower_dict = {}\n",
    "        \n",
    "        \n",
    "        upper_key_list = list(range(len(upper_img)))\n",
    "        lower_key_list = list(range(len(lower_img)))\n",
    "\n",
    "        # Only train on evenly split images\n",
    "        if lower_img == None:\n",
    "            num_subvolumes = len(upper_img)\n",
    "            for bindex in range(0, num_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = upper_key_list[bindex:num_subvolumes]\n",
    "                else:\n",
    "                    batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(output, 1)\n",
    "\n",
    "                # discretize probability values \n",
    "                prediction = torch.argmax(probabilities, 1)\n",
    "                tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "                # calculate the loss for the current batch, save the loss per epoch to calculate the average running loss\n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "\n",
    "            # lower list does not exist\n",
    "            tmp_lower_list = None\n",
    "\n",
    "        # train on both \n",
    "        else:\n",
    "            num_subvolumes = len(upper_img)\n",
    "            for bindex in range(0, num_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = upper_key_list[bindex:num_subvolumes]\n",
    "                else:\n",
    "                    batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs) # predict the batches\n",
    "                probabilities = torch.softmax(output, 1) \n",
    "                prediction = torch.argmax(probabilities,1)\n",
    "\n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "\n",
    "                # update the upper img dictionary\n",
    "                tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "            num_subvolumes = len(lower_img)\n",
    "            for bindex in range(0, num_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = lower_key_list[bindex:num_subvolumes]\n",
    "                else:\n",
    "                    batch_keys = lower_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([lower_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                sub_masks = torch.squeeze(torch.stack([lower_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(output, 1)\n",
    "                prediction = torch.argmax(probabilities,1)\n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "\n",
    "                # update the lower dictionary\n",
    "                tmp_lower_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "            # return tmp_upper_list, tmp_lower_list, running_loss / count\n",
    "    \n",
    "        # neuron reconstruction to calculate the dice metric.\n",
    "        orig_shape = full_mask.shape[1:-1]\n",
    "        reconstructed_mask_order = (3,0,1,2)\n",
    "\n",
    "\n",
    "        upper_values = torch.stack([tmp_upper_dict[key] for key in list(range(len(tmp_upper_dict)))])\n",
    "        lower_values = torch.stack([tmp_lower_dict[key] for key in list(range(len(tmp_lower_dict)))])\n",
    "\n",
    "\n",
    "        reconstructed = reconstruct_training_masks(upper_values, lower_values, upper_shape, \n",
    "                                                    lower_shape, patch_size, orig_shape) # returns (z,y,x)\n",
    "        reconstructed = to_categorical_torch(reconstructed, num_classes = 4) # returns (z,y,x,c)\n",
    "        reconstructed = torch.permute(reconstructed, reconstructed_mask_order)\n",
    "        reconstructed = torch.unsqueeze(reconstructed, 0) # make reconstructed image into (Batch,c,z,y,x)\n",
    "\n",
    "#         full_mask = full_mask.type(torch.int16).cpu()\n",
    "        gt_mask = torch.permute(full_mask, dim_order).cpu() # roll axis of grount truth mask into (batch,c,z,y,x)\n",
    "        \n",
    "    return {\"batch_loss\":running_loss/count_loss, \"y_pred\":reconstructed, \"y\":gt_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af1c2fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.022713Z",
     "iopub.status.busy": "2022-08-12T20:27:26.022444Z",
     "iopub.status.idle": "2022-08-12T20:27:26.028142Z",
     "shell.execute_reply": "2022-08-12T20:27:26.027443Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.022672Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Engine(train)\n",
    "evaluator = Engine(validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d87806",
   "metadata": {},
   "source": [
    "## Metrics and Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e791e49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.029418Z",
     "iopub.status.busy": "2022-08-12T20:27:26.029196Z",
     "iopub.status.idle": "2022-08-12T20:27:26.043265Z",
     "shell.execute_reply": "2022-08-12T20:27:26.042547Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.029392Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up progress bar\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.metrics import RunningAverage\n",
    "# from ignite.metrics import ConfusionMatrix, DiceCoefficient\n",
    "from monai.handlers.ignite_metric import IgniteMetric\n",
    "\n",
    "def metric_output_transform(output):\n",
    "    y_pred, y = output[\"y_pred\"], output[\"y\"]\n",
    "    return y_pred, y\n",
    "\n",
    "# cm = ConfusionMatrix(num_classes=4, output_transform = metric_output_transform)\n",
    "# dice_metric = RunningAverage(DiceCoefficient(cm))\n",
    "dice_metric = DiceMetric(include_background=False, reduction='mean_batch', get_not_nans = True)\n",
    "metric = IgniteMetric(dice_metric, output_transform=metric_output_transform)\n",
    "\n",
    "# progress output transform\n",
    "def loss_output_transform(output):\n",
    "    loss = output[\"batch_loss\"]\n",
    "    return loss\n",
    "\n",
    "# Attach both metric to trainer and evaluator engine\n",
    "metric.attach(trainer,\"Dice\")\n",
    "metric.attach(evaluator,\"Dice\")\n",
    "\n",
    "# RunningAverage(output_transform=loss_output_transform).attach(trainer, \"batch_loss\")\n",
    "RunningAverage(output_transform=loss_output_transform).attach(evaluator, \"batch_loss\")\n",
    "pbar = ProgressBar(persist=True)\n",
    "pbar.attach(trainer, metric_names=[\"batch_loss\"])\n",
    "pbar.attach(evaluator, metric_names=[\"batch_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734974d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3127f63e",
   "metadata": {},
   "source": [
    "## Setup Model and Log Saving Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac261d3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.044354Z",
     "iopub.status.busy": "2022-08-12T20:27:26.044139Z",
     "iopub.status.idle": "2022-08-12T20:27:26.050442Z",
     "shell.execute_reply": "2022-08-12T20:27:26.049725Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.044328Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"ResUNET\"\n",
    "date = datetime.now(tz=pytz.utc).strftime('%Y%m%d')\n",
    "time = datetime.now(tz=pytz.utc).strftime('%H%M%S')\n",
    "\n",
    "model_directory = f\"/home/jovyan/workspace/results/{model_name}/\"\n",
    "date_directory = f\"/{date}/\"\n",
    "time_directory = f\"{date}_{time}/\"\n",
    "log_directory = model_directory + date_directory + time_directory + \"log\"\n",
    "os.makedirs(log_directory)\n",
    "\n",
    "# create writer to log results into tensorboard\n",
    "log_writer = SummaryWriter(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de19a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.051627Z",
     "iopub.status.busy": "2022-08-12T20:27:26.051412Z",
     "iopub.status.idle": "2022-08-12T20:27:26.076196Z",
     "shell.execute_reply": "2022-08-12T20:27:26.075534Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.051601Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "@trainer.on(Events.STARTED)\n",
    "def print_start(trainer):\n",
    "    print(\"Training Started\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_STARTED)\n",
    "def print_epoch(trainer):\n",
    "    print(\"Epoch : {}\".format(trainer.state.epoch))\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_model(trainer):\n",
    "    global best_dice\n",
    "    global best_epoch\n",
    "    global best_epoch_file\n",
    "    global best_loss\n",
    "    \n",
    "    epoch = trainer.state.epoch\n",
    "    def get_saved_model_path(epoch):\n",
    "        return model_directory + date_directory + time_directory + f\"{model_name}_{epoch}.pth\"\n",
    "\n",
    "    # initialize global values\n",
    "    best_dice = -torch.inf if epoch == 1 else best_dice\n",
    "    best_loss = torch.inf if epoch == 1 else best_loss\n",
    "    best_epoch = 1 if epoch == 1 else best_epoch\n",
    "    best_epoch_file = '' if epoch == 1 else best_epoch_file\n",
    "    \n",
    "    def log_training_results(trainer):\n",
    "        evaluator.run(training_dataloader)\n",
    "        # Get engine metrics and losses\n",
    "        training_metrics = copy.deepcopy(evaluator.state.metrics)\n",
    "        pbar.log_message(\n",
    "            \"Training Results - Epoch: {} \\nMetrics\\n{}\"\n",
    "            .format(trainer.state.epoch, pprint.pformat(training_metrics)))\n",
    "        return training_metrics\n",
    "    \n",
    "    def log_testing_results(trainer):\n",
    "        evaluator.run(testing_dataloader)\n",
    "        testing_metrics = copy.deepcopy(evaluator.state.metrics)\n",
    "        scheduler.step(testing_metrics[\"batch_loss\"])\n",
    "        pbar.log_message(\n",
    "            \"Validation Results - Epoch: {} \\nMetrics\\n{}\"\n",
    "            .format(trainer.state.epoch, pprint.pformat(testing_metrics)))\n",
    "        return testing_metrics\n",
    "    \n",
    "    training_metrics= log_training_results(trainer)\n",
    "    testing_metrics= log_testing_results(trainer)\n",
    "    \n",
    "    train_dice = training_metrics['Dice']\n",
    "    val_dice = testing_metrics['Dice']\n",
    "\n",
    "    train_mean_dice = torch.mean(train_dice)\n",
    "    val_mean_dice = torch.mean(val_dice)\n",
    "    train_loss = training_metrics['batch_loss']\n",
    "    val_loss = testing_metrics['batch_loss']\n",
    "    \n",
    "\n",
    "    # log results\n",
    "    log_writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss}, epoch)\n",
    "    log_writer.add_scalars('Training vs. Validation Mean Dice ',\n",
    "                       {'Training Mean Dice' : train_mean_dice, 'Validation Mean Dice' : val_mean_dice}, epoch)\n",
    "    log_writer.add_scalars('Training vs. Validation Soma Dice ',\n",
    "                       {'Training Soma Dice' : train_dice[0], 'Validation Soma Dice' : val_dice[0]}, epoch)\n",
    "    log_writer.add_scalars('Training vs. Validation Dendrite Dice ',\n",
    "                       {'Training Dendrite Dice' : train_dice[1], 'Validation Dendrite Dice' : val_dice[1]}, epoch)\n",
    "    log_writer.add_scalars('Training vs. Validation Filopodias Dice ',\n",
    "                       {'Training Filopodias Dice' : train_dice[2], 'Validation Filopodias Dice' : val_dice[2]}, epoch)\n",
    "    log_writer.flush()\n",
    "\n",
    "    if (testing_metrics['batch_loss'] < best_loss):\n",
    "        \n",
    "        # if there was a previous model saved, delete that one\n",
    "        prev_best_epoch_file = get_saved_model_path(best_epoch)\n",
    "        if os.path.exists(prev_best_epoch_file):\n",
    "            os.remove(prev_best_epoch_file)\n",
    "\n",
    "        # update the best mean dice and loss and save the new model state\n",
    "#         best_dice = val_mean_dice\n",
    "        best_loss = testing_metrics['batch_loss']\n",
    "        best_epoch = epoch\n",
    "        best_epoch_file = get_saved_model_path(best_epoch)\n",
    "#         print(f'\\nEpoch: {best_epoch} - New best Dice and Loss! Mean Dice: {best_dice} Loss: {best_loss}\\n\\n\\n')\n",
    "        print(f'\\nEpoch: {best_epoch} - New best Loss! Loss: {best_loss}\\n\\n\\n')\n",
    "        torch.save(model.state_dict(), best_epoch_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1236e",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7644e20c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.077289Z",
     "iopub.status.busy": "2022-08-12T20:27:26.077075Z",
     "iopub.status.idle": "2022-08-12T20:27:26.083430Z",
     "shell.execute_reply": "2022-08-12T20:27:26.082728Z",
     "shell.execute_reply.started": "2022-08-12T20:27:26.077263Z"
    }
   },
   "outputs": [],
   "source": [
    "# from ignite.handlers import EarlyStopping\n",
    "\n",
    "# def score_function(engine):\n",
    "#     val_loss = engine.state.metrics['batch_loss']\n",
    "#     return -val_loss\n",
    "\n",
    "# handler = EarlyStopping(patience=10, score_function=score_function, min_delta=1e-6, trainer=trainer)\n",
    "# evaluator.add_event_handler(Events.COMPLETED, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38cbc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T20:27:26.084513Z",
     "iopub.status.busy": "2022-08-12T20:27:26.084298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running Training Engine\n",
    "\n",
    "trainer.run(training_dataloader, max_epochs = max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92def897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25982e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e948cd2eddc2b56aed0b51f92bfb3429aca2637a323db441b1bbdcb5065963e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
