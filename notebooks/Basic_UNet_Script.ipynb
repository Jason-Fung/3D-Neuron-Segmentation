{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8c7679",
   "metadata": {},
   "source": [
    "## Import Modules and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c18d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:32.401902Z",
     "iopub.status.busy": "2022-07-11T18:06:32.401531Z",
     "iopub.status.idle": "2022-07-11T18:06:36.170717Z",
     "shell.execute_reply": "2022-07-11T18:06:36.169886Z",
     "shell.execute_reply.started": "2022-07-11T18:06:32.401810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from processing_functions import *\n",
    "\n",
    "# import deep learning libraries\n",
    "from torchvision import transforms, utils\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "\n",
    "# import processing libraries\n",
    "from patchify import unpatchify\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4c13be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:36.919938Z",
     "iopub.status.busy": "2022-07-11T18:06:36.919622Z",
     "iopub.status.idle": "2022-07-11T18:06:36.924139Z",
     "shell.execute_reply": "2022-07-11T18:06:36.923374Z",
     "shell.execute_reply.started": "2022-07-11T18:06:36.919906Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66751d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:37.506043Z",
     "iopub.status.busy": "2022-07-11T18:06:37.505739Z",
     "iopub.status.idle": "2022-07-11T18:06:37.510183Z",
     "shell.execute_reply": "2022-07-11T18:06:37.509488Z",
     "shell.execute_reply.started": "2022-07-11T18:06:37.506014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # cloud server\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# macbook\n",
    "# # use_mps = torch.has_mps\n",
    "# use_mps = False\n",
    "# device = torch.device(\"mps\" if use_mps else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3a4589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:37.922173Z",
     "iopub.status.busy": "2022-07-11T18:06:37.921866Z",
     "iopub.status.idle": "2022-07-11T18:06:37.933249Z",
     "shell.execute_reply": "2022-07-11T18:06:37.932497Z",
     "shell.execute_reply.started": "2022-07-11T18:06:37.922144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f48ac",
   "metadata": {},
   "source": [
    "## Read TIF images into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5998d500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:38.656986Z",
     "iopub.status.busy": "2022-07-11T18:06:38.656679Z",
     "iopub.status.idle": "2022-07-11T18:06:38.886896Z",
     "shell.execute_reply": "2022-07-11T18:06:38.886138Z",
     "shell.execute_reply.started": "2022-07-11T18:06:38.656957Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import tifffile\n",
    "import glob\n",
    "\n",
    "# Cloud Server\n",
    "raw_path = \"/home/jovyan/workspace/Images/Raw/*.tif\"\n",
    "mask_path = \"/home/jovyan/workspace/Images/Mask/*.tif\"\n",
    "\n",
    "# # Jason's Desktop\n",
    "# raw_path = \"H:\\My Drive\\Raw\\*.tif\"\n",
    "# mask_path = \"H:\\My Drive\\Mask\\*.tif\"\n",
    "\n",
    "# # Jason's Macbook\n",
    "# raw_path = \"/Users/jasonfung/haaslabdataimages@gmail.com - Google Drive/My Drive/Raw/*.tif\"\n",
    "# mask_path = \"/Users/jasonfung/haaslabdataimages@gmail.com - Google Drive/My Drive/Mask/*.tif\"\n",
    "\n",
    "raw_filename_list = glob.glob(raw_path) \n",
    "mask_filename_list = glob.glob(mask_path)\n",
    "\n",
    "# Pre Shuffle\n",
    "raw_filename_list.sort()\n",
    "mask_filename_list.sort()\n",
    "\n",
    "# Shuffle the filename list\n",
    "from sklearn.utils import shuffle\n",
    "raw_filename_list, mask_filename_list = shuffle(raw_filename_list, mask_filename_list, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fc2e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:38.937691Z",
     "iopub.status.busy": "2022-07-11T18:06:38.937403Z",
     "iopub.status.idle": "2022-07-11T18:06:38.942668Z",
     "shell.execute_reply": "2022-07-11T18:06:38.941995Z",
     "shell.execute_reply.started": "2022-07-11T18:06:38.937662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/workspace/Images/Raw/000_ML_20190604_B_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_ML_20180614_N1_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_B_181107_A_N1B2_4a61736f.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_ML_20180614_N5_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180621_A_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20190604_A_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_D_180907_A_N1B3_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_D_180906_A_N1C1_53696168.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/060_B_181031_A_N1B3_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180613_N4_4a61736f.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180622_N2_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/00_GFPC_20180615_N1_50616967.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_GFP_181027_52616a61.tif',\n",
       " '/home/jovyan/workspace/Images/Raw/000_ML_20180622_N1_53696168.tif']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f282f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:39.336584Z",
     "iopub.status.busy": "2022-07-11T18:06:39.336276Z",
     "iopub.status.idle": "2022-07-11T18:06:39.341375Z",
     "shell.execute_reply": "2022-07-11T18:06:39.340657Z",
     "shell.execute_reply.started": "2022-07-11T18:06:39.336554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_filename_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77708ef1",
   "metadata": {},
   "source": [
    "## Processing Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17beee8",
   "metadata": {},
   "source": [
    "## Patching and Reconstruction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e13bf",
   "metadata": {},
   "source": [
    "Input of an image is (# z stacks, # x pixels, # y pixels). Then it is split into sub volumes of size (z patch size, x patch size, y patch size) where each subvolume is (voxelized), meaning it has a coordinate relative to the original image. This ends up becoming a (z location, x location, y location, z patch size, x patch size, y patch size).\n",
    "\n",
    "If the image has a depth that cannot be evenly split by 2^n depth patch size where n is an integer, the function patch_images() will return an \"upper half\" and a \"lower half\" of the image. For example, given an image size of (77,512,512) and a z patch size of 16, x patch size of 32, and y patch of 32, the image cannot be evenly split into 16ths, since 77 % 16 = 4 remainder 13. Therefore, starting from the top of the stack to slice 64: the upper half becomes (64, 512, 512). Contrastly, starting from the bottom of the stack at slice 77 to slice 61, the lower half becomes (16, 512, 512). The two split volumes will become merged in the end.\n",
    "\n",
    "To prepare for training, the coordinated subvolumes are reshaped into \"batch form\". From the previous example with the upper half (64, 512, 512) split and coordinated into a 6D array (4, 16, 16, 16, 32, 32) -> (4 * 16 * 16, 16, 32, 32).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d07dde",
   "metadata": {},
   "source": [
    "## Split Training and Testing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6736d5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:42.056522Z",
     "iopub.status.busy": "2022-07-11T18:06:42.056214Z",
     "iopub.status.idle": "2022-07-11T18:06:42.065995Z",
     "shell.execute_reply": "2022-07-11T18:06:42.065148Z",
     "shell.execute_reply.started": "2022-07-11T18:06:42.056491Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# define patching parameters\n",
    "lateral_steps = 64\n",
    "axial_steps = 16\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "split_size = 0.8\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "#                                       new_shape(new_xy = (600,960)),\n",
    "                                      MinMaxScalerVectorized(),\n",
    "                                      patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = False)])\n",
    "\n",
    "# define transforms for labeled masks\n",
    "label_transforms = transforms.Compose([\n",
    "#                                        new_shape(new_xy = (600,960)),\n",
    "                                       process_masks(int_class = 3),\n",
    "                                       patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = True)])\n",
    "\n",
    "\n",
    "raw_training_list, mask_training_list = raw_filename_list[:int(split_size*len(raw_filename_list))], mask_filename_list[:int(split_size*len(mask_filename_list))]\n",
    "raw_testing_list, mask_testing_list = raw_filename_list[int(split_size*len(raw_filename_list)):], mask_filename_list[int(split_size*len(mask_filename_list)):]\n",
    "print(len(raw_training_list))\n",
    "\n",
    "training_data = MyImageDataset(raw_training_list,\n",
    "                               mask_training_list,\n",
    "                               transform = patch_transform,\n",
    "                               label_transform = label_transforms,\n",
    "                               device = device,\n",
    "                               img_order = dim_order,\n",
    "                               mask_order = dim_order,\n",
    "                               num_classes = 4,\n",
    "                               train=True)\n",
    "\n",
    "testing_data = MyImageDataset(raw_testing_list,\n",
    "                              mask_testing_list,\n",
    "                              transform = patch_transform,\n",
    "                              label_transform = label_transforms,\n",
    "                              device = device,\n",
    "                              img_order = dim_order,\n",
    "                              mask_order = dim_order,\n",
    "                              num_classes = 4,\n",
    "                              train=False)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "training_dataloader = DataLoader(training_data, batch_size = 1, shuffle = True)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565e4f6",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f719cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:07:58.375444Z",
     "iopub.status.busy": "2022-07-11T18:07:58.375120Z",
     "iopub.status.idle": "2022-07-11T18:07:58.379835Z",
     "shell.execute_reply": "2022-07-11T18:07:58.379108Z",
     "shell.execute_reply.started": "2022-07-11T18:07:58.375412Z"
    }
   },
   "outputs": [],
   "source": [
    "# from monai.losses import DiceLoss\n",
    "from monai.losses import FocalLoss, DiceFocalLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import BasicUNet, UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243ebd3",
   "metadata": {},
   "source": [
    "## Model and Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6e021",
   "metadata": {},
   "source": [
    "### UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4272ba30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:06:46.853943Z",
     "iopub.status.busy": "2022-07-11T18:06:46.853616Z",
     "iopub.status.idle": "2022-07-11T18:06:50.366811Z",
     "shell.execute_reply": "2022-07-11T18:06:50.366041Z",
     "shell.execute_reply.started": "2022-07-11T18:06:46.853911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (16, 32, 64, 128, 256, 16).\n"
     ]
    }
   ],
   "source": [
    "# set up loss and optimizer\n",
    "max_epochs = 250\n",
    "dropout = 0.10\n",
    "learning_rate = 5e-5\n",
    "decay = 1e-5\n",
    "input_chnl = 1\n",
    "output_chnl = 4\n",
    "\n",
    "model = BasicUNet(spatial_dims=3, \n",
    "                  in_channels = input_chnl,\n",
    "                  out_channels = output_chnl,\n",
    "                  features = (16, 32, 64, 128, 256, 16),\n",
    "                  norm = \"batch\",\n",
    "                  dropout = dropout,\n",
    "               )\n",
    "model = model.to(device)\n",
    "\n",
    "# loss_function = FocalLoss()\n",
    "loss_function = DiceCELoss()\n",
    "dice = DiceMetric()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "# Instantiate Dice metric\n",
    "dice = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans = True)\n",
    "discretize = Compose([Activations(softmax = True), \n",
    "                      AsDiscrete(logit_thresh=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0d03fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T18:08:11.382589Z",
     "iopub.status.busy": "2022-07-11T18:08:11.382263Z",
     "iopub.status.idle": "2022-07-11T18:08:11.388320Z",
     "shell.execute_reply": "2022-07-11T18:08:11.387525Z",
     "shell.execute_reply.started": "2022-07-11T18:08:11.382558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4810977\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf6512",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85533b5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-07T23:01:39.332923Z",
     "iopub.status.idle": "2022-07-07T23:01:39.333294Z",
     "shell.execute_reply": "2022-07-07T23:01:39.333138Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Augmentation Function parameters\n",
    "# degree = (25, 5, 5)\n",
    "# translate = (10,10,10)\n",
    "# transform_rotate = torchio.RandomAffine(degrees=degree, translation=translate, image_interpolation=\"bspline\")\n",
    "# transform_flip = torchio.RandomFlip(axes=('ap',))\n",
    "# all_transform = torchio.Compose([transform_rotate,\n",
    "#                                  transform_flip])\n",
    "\n",
    "# Augmentation Function parameters using resnet parameters\n",
    "degree = (25, 5, 5)\n",
    "# translate = (10,10,10)\n",
    "transform_rotate = torchio.RandomAffine(degrees=degree, \n",
    "#                                         translation=translate, \n",
    "                                        image_interpolation=\"bspline\")\n",
    "transform_flip = torchio.RandomFlip(axes=('ap',))\n",
    "all_transform = torchio.Compose([transform_rotate,\n",
    "                                 transform_flip])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dee372",
   "metadata": {},
   "source": [
    "## Compose Dice Metric and Discretize Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b343109",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-07T23:01:39.334184Z",
     "iopub.status.idle": "2022-07-07T23:01:39.334498Z",
     "shell.execute_reply": "2022-07-07T23:01:39.334345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize metric lists to store Dice metrics\n",
    "mean_dice = []\n",
    "dice_soma = []\n",
    "dice_dendrite = []\n",
    "dice_filopodias = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714db00",
   "metadata": {},
   "source": [
    "## Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528f396",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-07T23:01:39.335493Z",
     "iopub.status.idle": "2022-07-07T23:01:39.335810Z",
     "shell.execute_reply": "2022-07-07T23:01:39.335657Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(batch_size, model, patch_size, augment = True, shuffle = True):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    count_loss = 0\n",
    "    # Instantiate the dice sum for each class\n",
    "    dice_mean = dice_background = dice_soma = dice_dendrite = dice_filopodias = 0.0\n",
    "    dice_count = 0\n",
    "    img_num = 0\n",
    "    \n",
    "    for upper_img, upper_shape, lower_img, lower_shape, full_mask, upper_mask, lower_mask in training_dataloader:\n",
    "        \n",
    "        img_num += 1\n",
    "        print(\"Training Image: \", img_num)\n",
    "        # Empty list to place subvolumes in\n",
    "        tmp_upper_dict = {}\n",
    "        tmp_lower_dict = {}    \n",
    "        \n",
    "        if shuffle == True:\n",
    "            # shuffle the batches\n",
    "            upper_key_list = list(range(len(upper_img)))\n",
    "            random.shuffle(upper_key_list)\n",
    "            \n",
    "            # check if lower img exists, otherwise perform shuffling\n",
    "            if lower_img == None:\n",
    "                pass\n",
    "            else:\n",
    "                lower_key_list = list(range(len(lower_img)))\n",
    "                random.shuffle(upper_key_list)\n",
    "        else:\n",
    "            upper_key_list = list(range(len(upper_img)))\n",
    "            lower_key_list = list(range(len(lower_img)))\n",
    "        \n",
    "        \n",
    "        # Only train on evenly split images\n",
    "        if lower_img == None:\n",
    "            num_subvolumes = len(upper_img)\n",
    "            for bindex in trange(0, num_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = upper_key_list[bindex:num_sub_volumes]\n",
    "                else:\n",
    "                    batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "               \n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(ouput, 1)\n",
    "                \n",
    "                # discretize probability values \n",
    "                prediction = torch.argmax(probabilities, 1)\n",
    "                tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "                \n",
    "                # calculate the loss for the current batch, save the loss per epoch to calculate the average running loss\n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                \n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += current_loss.item()\n",
    "                dice_count += 1\n",
    "            \n",
    "            # lower list does not exist\n",
    "            tmp_lower_list = None\n",
    "                \n",
    "        # train on both \n",
    "        else:\n",
    "            print(\"Training Upper Half of Image\")\n",
    "            num_upper_subvolumes = len(upper_img)\n",
    "            if augment:\n",
    "                print(\"Augmenting Images\")\n",
    "                upper_indexes = get_index_nonempty_cubes(upper_mask)\n",
    "\n",
    "                for bindex in trange(0, len(upper_indexes), batch_size):\n",
    "                    # for augmentation\n",
    "                    if bindex + batch_size > len(upper_indexes):\n",
    "                        upper_batch = upper_indexes[bindex:len(upper_indexes)]\n",
    "                    else:\n",
    "                        upper_batch = upper_indexes[bindex:bindex+batch_size]\n",
    "                        \n",
    "                    sub_imgs, sub_masks = augmentation(all_transform, upper_img, upper_mask, upper_batch)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(sub_imgs)\n",
    "                    probabilities = torch.softmax(output, 1)\n",
    "                    prediction = torch.argmax(probabilities, 1)\n",
    "                    \n",
    "                    current_loss = loss_function(probabilities, sub_masks)\n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()\n",
    "                    count_loss += 1\n",
    "            \n",
    "            \n",
    "            for bindex in trange(0, num_upper_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_upper_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = upper_key_list[bindex:num_upper_subvolumes]\n",
    "                else:\n",
    "                    batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs) # predict the batches\n",
    "                probabilities = torch.softmax(output, 1) \n",
    "                prediction = torch.argmax(probabilities,1)\n",
    "                \n",
    "                # update the upper img dictionary\n",
    "                tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "                \n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                \n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "            \n",
    "            print(\"Training Lower Half of Image\")\n",
    "            num_lower_subvolumes = len(lower_img)\n",
    "            if augment:\n",
    "                print(\"Augmenting Lower Half Images\")\n",
    "                \n",
    "                lower_indexes = get_index_nonempty_cubes(lower_mask)\n",
    "                \n",
    "                for bindex in trange(0, len(lower_indexes), batch_size):\n",
    "                    # for augmentation\n",
    "                    if bindex + batch_size > len(lower_indexes):\n",
    "                        lower_batch = lower_indexes[bindex:len(lower_indexes)]\n",
    "                    else:\n",
    "                        lower_batch = lower_indexes[bindex:bindex+batch_size]\n",
    "                        \n",
    "                    sub_imgs, sub_masks = augmentation(all_transform, upper_img, upper_mask, upper_batch)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(sub_imgs)\n",
    "                    probabilities = torch.softmax(output, 1)\n",
    "                    prediction = torch.argmax(probabilities, 1)\n",
    "                    \n",
    "                    current_loss = loss_function(probabilities, sub_masks)\n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()\n",
    "                    count_loss += 1\n",
    "                    \n",
    "            print(\"Non Augmented Image\")\n",
    "            for bindex in trange(0, num_lower_subvolumes, batch_size):\n",
    "                if bindex + batch_size > num_lower_subvolumes:\n",
    "                    # if the bindex surpasses the number of number of sub volumes\n",
    "                    batch_keys = lower_key_list[bindex:num_lower_subvolumes]\n",
    "                else:\n",
    "                    batch_keys = lower_key_list[bindex:bindex+batch_size]\n",
    "                \n",
    "                sub_imgs = torch.squeeze(torch.stack([lower_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                sub_masks = torch.squeeze(torch.stack([lower_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(sub_imgs)\n",
    "                probabilities = torch.softmax(output, 1)\n",
    "                prediction = torch.argmax(probabilities,1)\n",
    "                \n",
    "                # update the lower dictionary\n",
    "                tmp_lower_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "                current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                \n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += current_loss.item()\n",
    "                count_loss += 1\n",
    "\n",
    "        # neuron reconstruction to calculate the dice metric.\n",
    "        orig_shape = full_mask.shape[1:-1]\n",
    "        reconstructed_mask_order = (3,0,1,2)\n",
    "        \n",
    "        upper_values = torch.stack([tmp_upper_dict[key] for key in list(range(len(tmp_upper_dict)))])\n",
    "        lower_values = torch.stack([tmp_lower_dict[key] for key in list(range(len(tmp_lower_dict)))])\n",
    "        \n",
    "        \n",
    "        reconstructed = reconstruct_training_masks(upper_values, lower_values, upper_shape, \n",
    "                                                   lower_shape, patch_size, orig_shape) # returns (z,y,x)\n",
    "        reconstructed = to_categorical_torch(reconstructed, num_classes = 4) # returns (z,y,x,c)\n",
    "        reconstructed = torch.permute(reconstructed, reconstructed_mask_order)\n",
    "        reconstructed = torch.unsqueeze(reconstructed, 0) # make reconstructed image into (Batch,c,z,y,x)\n",
    "        \n",
    "        gt_mask = torch.permute(full_mask, dim_order).cpu() # roll axis of grount truth mask into (batch,c,z,y,x)\n",
    "        \n",
    "        # compute the dice score for each class\n",
    "        scores = dice(reconstructed, gt_mask)\n",
    "        scores = scores[0]\n",
    "        dice_mean += scores.mean()\n",
    "        dice_background += scores[0]\n",
    "        dice_soma += scores[1]\n",
    "        dice_dendrite += scores[2]\n",
    "        dice_filopodias += scores[3]\n",
    "        \n",
    "        dice_count += 1 \n",
    "        \n",
    "#         print(f'Training Dice: Mean = {dice_mean/dice_count}, Background = {dice_background/dice_count}, Soma = {dice_soma/dice_count}, Dendrite = {dice_dendrite/dice_count}, Filopodias = {dice_filopodias/dice_count}')\n",
    "#         print(f'Training loss: {running_loss / count_loss}')\n",
    "#         # print(f'Training Dice: {running_dice / count}')\n",
    "#         writer.add_scalar('Training batch loss', running_loss / count_loss)\n",
    "#         writer.flush()\n",
    "    \n",
    "#     scheduler.step()\n",
    "    running_dice = [dice_background/dice_count, dice_soma/dice_count, dice_dendrite/dice_count, dice_filopodias/dice_count]\n",
    "        \n",
    "    return running_dice, running_loss / count_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90603d5f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-07T23:01:39.336796Z",
     "iopub.status.idle": "2022-07-07T23:01:39.337157Z",
     "shell.execute_reply": "2022-07-07T23:01:39.336960Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(batch_size, model, patch_size, shuffle = False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        count_loss = 0\n",
    "        # Instantiate the dice sum for each class\n",
    "        dice_mean = dice_background = dice_soma = dice_dendrite = dice_filopodias = 0.0\n",
    "        dice_count = 0\n",
    "        img_num = 0\n",
    "\n",
    "        for upper_img, upper_shape, lower_img, lower_shape, full_mask, upper_mask, lower_mask in testing_dataloader:\n",
    "            img_num += 1\n",
    "            print(\"Training Image: \", img_num)\n",
    "            # Empty list to place subvolumes in\n",
    "            tmp_upper_dict = {}\n",
    "            tmp_lower_dict = {}    \n",
    "\n",
    "            if shuffle == True:\n",
    "                # shuffle the batches\n",
    "                upper_key_list = list(range(len(upper_img)))\n",
    "                random.shuffle(upper_key_list)\n",
    "\n",
    "                # check if lower img exists, otherwise perform shuffling\n",
    "                if lower_img == None:\n",
    "                    pass\n",
    "                else:\n",
    "                    lower_key_list = list(range(len(lower_img)))\n",
    "                    random.shuffle(upper_key_list)\n",
    "            else:\n",
    "                upper_key_list = list(range(len(upper_img)))\n",
    "                lower_key_list = list(range(len(lower_img)))\n",
    "\n",
    "\n",
    "            # Only train on evenly split images\n",
    "            if lower_img == None:\n",
    "                num_subvolumes = len(upper_img)\n",
    "                for bindex in trange(0, num_subvolumes, batch_size):\n",
    "                    if bindex + batch_size > num_subvolumes:\n",
    "                        # if the bindex surpasses the number of number of sub volumes\n",
    "                        batch_keys = upper_key_list[bindex:num_sub_volumes]\n",
    "                    else:\n",
    "                        batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                    \n",
    "                    sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                    sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(sub_imgs)\n",
    "                    probabilities = torch.softmax(ouput, 1)\n",
    "\n",
    "                    # discretize probability values \n",
    "                    prediction = torch.argmax(probabilities, 1)\n",
    "                    tmp_upper_dict.update(dict(zip(batch_keys,sub_prediction)))\n",
    "\n",
    "                    # calculate the loss for the current batch, save the loss per epoch to calculate the average running loss\n",
    "                    current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                    running_loss += current_loss.item()\n",
    "                    dice_count += 1\n",
    "\n",
    "                # lower list does not exist\n",
    "                tmp_lower_list = None\n",
    "\n",
    "            # train on both \n",
    "            else:\n",
    "                print(\"Validating Upper Half of Image\")\n",
    "                num_subvolumes = len(upper_img)\n",
    "                for bindex in trange(0, num_subvolumes, batch_size):\n",
    "                    if bindex + batch_size > num_subvolumes:\n",
    "                        # if the bindex surpasses the number of number of sub volumes\n",
    "                        batch_keys = upper_key_list[bindex:num_sub_volumes]\n",
    "                    else:\n",
    "                        batch_keys = upper_key_list[bindex:bindex+batch_size]\n",
    "                    \n",
    "                    sub_imgs = torch.squeeze(torch.stack([upper_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                    sub_masks = torch.squeeze(torch.stack([upper_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(sub_imgs) # predict the batches\n",
    "                    probabilities = torch.softmax(output, 1) \n",
    "                    prediction = torch.argmax(probabilities,1)\n",
    "\n",
    "                    # update the upper img dictionary\n",
    "                    tmp_upper_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "                    current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                    running_loss += current_loss.item()\n",
    "                    count_loss += 1\n",
    "\n",
    "                print(\"Validating Lower Half of Image\")\n",
    "                num_subvolumes = len(lower_img)\n",
    "                for bindex in trange(0, num_subvolumes, batch_size):\n",
    "                    if bindex + batch_size > num_subvolumes:\n",
    "                        # if the bindex surpasses the number of number of sub volumes\n",
    "                        batch_keys = lower_key_list[bindex:num_sub_volumes]\n",
    "                    else:\n",
    "                        batch_keys = lower_key_list[bindex:bindex+batch_size]\n",
    "                    \n",
    "                    sub_imgs = torch.squeeze(torch.stack([lower_img.get(key) for key in batch_keys], dim=1), dim = 0) \n",
    "                    sub_masks = torch.squeeze(torch.stack([lower_mask.get(key) for key in batch_keys], dim=1), dim = 0)\n",
    "\n",
    "                    output = model(sub_imgs)\n",
    "                    probabilities = torch.softmax(output, 1)\n",
    "                    prediction = torch.argmax(probabilities,1)\n",
    "\n",
    "                    # update the lower dictionary\n",
    "                    tmp_lower_dict.update(dict(zip(batch_keys,prediction)))\n",
    "\n",
    "                    current_loss = loss_function(probabilities, sub_masks) # + dice_loss(predictions, patch_gt)\n",
    "                    running_loss += current_loss.item()\n",
    "                    count_loss += 1\n",
    "\n",
    "                # return tmp_upper_list, tmp_lower_list, running_loss / count\n",
    "        \n",
    "            # neuron reconstruction to calculate the dice metric.\n",
    "            orig_shape = full_mask.shape[1:-1]\n",
    "            reconstructed_mask_order = (3,0,1,2)\n",
    "\n",
    "\n",
    "            upper_values = torch.stack([tmp_upper_dict[key] for key in list(range(len(tmp_upper_dict)))])\n",
    "            lower_values = torch.stack([tmp_lower_dict[key] for key in list(range(len(tmp_lower_dict)))])\n",
    "\n",
    "\n",
    "            reconstructed = reconstruct_training_masks(upper_values, lower_values, upper_shape, \n",
    "                                                       lower_shape, patch_size, orig_shape) # returns (z,y,x)\n",
    "            reconstructed = to_categorical_torch(reconstructed, num_classes = 4) # returns (z,y,x,c)\n",
    "            reconstructed = torch.permute(reconstructed, reconstructed_mask_order)\n",
    "            reconstructed = torch.unsqueeze(reconstructed, 0) # make reconstructed image into (Batch,c,z,y,x)\n",
    "\n",
    "            gt_mask = torch.permute(full_mask, dim_order).cpu() # roll axis of grount truth mask into (batch,c,z,y,x)\n",
    "\n",
    "            # compute the dice score for each class\n",
    "            scores = dice(reconstructed, gt_mask)\n",
    "            scores = scores[0]\n",
    "            dice_mean += scores.mean()\n",
    "            dice_background += scores[0]\n",
    "            dice_soma += scores[1]\n",
    "            dice_dendrite += scores[2]\n",
    "            dice_filopodias += scores[3]\n",
    "\n",
    "            dice_count += 1 \n",
    "\n",
    "    #         print(f'Training Dice: Mean = {dice_mean/dice_count}, Background = {dice_background/dice_count}, Soma = {dice_soma/dice_count}, Dendrite = {dice_dendrite/dice_count}, Filopodias = {dice_filopodias/dice_count}')\n",
    "    #         print(f'Training loss: {running_loss / count_loss}')\n",
    "    #         # print(f'Training Dice: {running_dice / count}')\n",
    "    #         writer.add_scalar('Training batch loss', running_loss / count_loss)\n",
    "    #         writer.flush()\n",
    "\n",
    "#         scheduler.step()\n",
    "        \n",
    "        running_dice = [dice_background/dice_count, dice_soma/dice_count, dice_dendrite/dice_count, dice_filopodias/dice_count]\n",
    "        \n",
    "        return running_dice, running_loss / count_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3ef45",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-07T23:01:39.337905Z",
     "iopub.status.idle": "2022-07-07T23:01:39.338216Z",
     "shell.execute_reply": "2022-07-07T23:01:39.338064Z"
    }
   },
   "outputs": [],
   "source": [
    "# # macbook log directory\n",
    "# log_directory = '/Users/jasonfung/Documents/Masters Project/Results/runs/unet/trainer_{}'\n",
    "\n",
    "# windows log directory\n",
    "# log_directory = \"\\\\Users\\\\Fungj\\\\Documents\\\\Results_{}\"\n",
    "\n",
    "# # cloud log and model directory\n",
    "log_directory = \"/home/jovyan/workspace/results/logs/unet/trainer_{}\" \n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(log_directory.format(timestamp))\n",
    "\n",
    "output_dir = '/home/jovyan/workspace/results/models/unet/'\n",
    "\n",
    "\n",
    "best_val_loss = np.inf\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f' Epoch {epoch}: ')\n",
    "    train_dice, train_loss = train(batch_size, model, patch_size)\n",
    "    val_dice, val_loss = validate(batch_size, model, patch_size)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}: Loss train {train_loss}; Dice train {train_dice}; validation {val_loss}; validation IOU {val_dice}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss}, epoch)\n",
    "    writer.add_scalars('Training vs. Validation Mean Dice ',\n",
    "                       {'Training Mean Dice' : np.mean(train_dice), 'Validation Mean Dice' : np.mean(val_dice)}, epoch)\n",
    "    writer.add_scalars('Training vs. Validation Soma Dice ',\n",
    "                       {'Training Soma Dice' : train_dice[1], 'Validation Soma Dice' : val_dice[1]}, epoch)\n",
    "    writer.add_scalars('Training vs. Validation Dendrite Dice ',\n",
    "                       {'Training Dendrite Dice' : train_dice[2], 'Validation Dendrite Dice' : val_dice[2]}, epoch)\n",
    "    writer.add_scalars('Training vs. Validation Filopodias Dice ',\n",
    "                       {'Training Filopodias Dice' : train_dice[3], 'Validation Filopodias Dice' : val_dice[3]}, epoch)\n",
    "    writer.flush()\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = output_dir + f'model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
