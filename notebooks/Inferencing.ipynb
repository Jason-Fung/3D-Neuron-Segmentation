{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219919d9",
   "metadata": {},
   "source": [
    "## Inferencing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f678db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import core libaries\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import tifffile\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__vsc_ipynb_file__))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "from src.processing.processing_functions import *\n",
    "\n",
    "# get working directory\n",
    "path = os.getcwd()\n",
    "sys.path.append(path)\n",
    "\n",
    "# import machine learning libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from monai.inferers.inferer import SlidingWindowInferer, SliceInferer\n",
    "from monai.networks.nets import BasicUNet, UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55de52bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eda705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"+s+d+f_ResUNet.onnx\"\n",
    "model_soma_dendrite = \"Soma+Dendrite.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e681a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing raw image\n",
    "lateral_steps = 64\n",
    "axial_steps = 16\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "batch_size = 64\n",
    "# split_size = 0.9\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "raw_path = filedialog.askopenfilename()\n",
    "raw_img = glob.glob(raw_path)\n",
    "orig_shape = tifffile.imread(raw_img).shape\n",
    "\n",
    "# Use patch transform to normalize and transform ndarray(z,y,x) -> tensor(\n",
    "patch_transform = transforms.Compose([MinMaxScalerVectorized(),\n",
    "                                      patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = False)])\n",
    "\n",
    "\n",
    "processed_test_img = MyImageDataset(raw_list = raw_img,\n",
    "                                    mask_list = None,\n",
    "                                    transform = patch_transform,\n",
    "                                    device = device,\n",
    "                                    mask_order = dim_order,\n",
    "                                    num_classes = None,\n",
    "                                    train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87454",
   "metadata": {},
   "source": [
    "## Using Custom Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d62d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstructed_img = inference(processed_test_img, \n",
    "                              model, \n",
    "                              batch_size, \n",
    "                              patch_size, \n",
    "                              orig_shape,\n",
    "                              )\n",
    "\n",
    "np.unique(reconstructed_img)\n",
    "\n",
    "if len(np.unique(reconstructed_img))-1 == 2:\n",
    "    reconstructed_img[reconstructed_img==1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reconstructed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(f'{raw_path}_+s+d+f.tif', reconstructed_img.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859310d",
   "metadata": {},
   "source": [
    "## Using MONAI Sliding Window Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c610e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f7aad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"+s_+d_-f\"\n",
    "model_soma_dendrite = \"+s_+d_-f_ResUNet_3_121.pth\"\n",
    "model_state = f\"C:\\\\Users\\\\Fungj\\\\Google Drive\\\\Masters Project\\\\Segmentation-Model\\\\3D-Neuron-Segment\\\\results\\ResUNET\\\\20230202\\\\16_128_128_+s_+d_-f\\\\{model_soma_dendrite}\"\n",
    "yaml_file = f\"C:\\\\Users\\\\Fungj\\\\Google Drive\\\\Masters Project\\\\Segmentation-Model\\\\3D-Neuron-Segment\\\\results\\\\ResUNET\\\\20230202\\\\16_128_128_+s_+d_-f\\\\20230202_143720_3D_ResUNet_experiment.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40dbf3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATASET': {'AUGMENTATION': {'augment': True, 'gamma_lower': -0.5, 'gamma_upper': 0.6, 'mean_noise': 0.05, 'std_noise': 0.025, 'x_deg': 0, 'y_deg': 0, 'z_deg': 25}, 'artifacts': [7], 'axial_steps': 16, 'batch_size': 64, 'ex_autofluorescence': False, 'ex_melanocytes': True, 'exp': '+s_+d_-f', 'folds': 4, 'lateral_steps': 128, 'remove_artifacts': False, 'x_patch': 128, 'y_patch': 128, 'z_patch': 16}, 'MODEL': {'channel_layers': [32, 64, 128, 256, 512], 'dropout': 0.15, 'input_dim': 1, 'l2': 0.0, 'learning_rate': 7.54e-05, 'model_arch': 'UNET', 'norm': 'instance', 'num_res_units': 2, 'spatial_dim': 3, 'strides': [2, 2, 2, 2]}, 'RESULTS': {'log_file_path': '/home/jsfung/projects/def-haas/jsfung/results/ResUNet/20230202/16_128_128_+s_+d_-f/143720_log_fold_2', 'model_states_path': '/home/jsfung/projects/def-haas/jsfung/results/ResUNet/20230202/16_128_128_+s_+d_-f/+s_+d_-f_ResUNet_2_149.pth'}, 'TRAINING': {'shuffle': True}, 'date': 'now', 'end_cycle': 20, 'loss': 'dice', 'mask_path': '/home/jsfung/projects/def-haas/jsfung/Images/new_labels/Mask/*.tif', 'max_epochs': 150, 'model_name': 'ResUNet', 'parent_dir': '/home/haas/projects/def-haas/jsfung', 'raw_path': '/home/jsfung/projects/def-haas/jsfung/Images/new_labels/Raw/*.tif', 'weights': None}\n"
     ]
    }
   ],
   "source": [
    "with open(yaml_file) as f:\n",
    "    config = yaml.load(f, Loader=SafeLoader)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4e5651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lateral_steps = config['DATASET']['lateral_steps']\n",
    "axial_steps = config['DATASET']['axial_steps']\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "batch_size = config['DATASET']['batch_size']\n",
    "input_chnl = 1\n",
    "output_chnl = 4\n",
    "norm_type = config['MODEL']['norm']\n",
    "dropout = 0.1\n",
    "\n",
    "model = UNet(spatial_dims=3, \n",
    "            in_channels = input_chnl,\n",
    "            out_channels = output_chnl,\n",
    "            channels = config['MODEL']['channel_layers'],\n",
    "            strides=config['MODEL']['strides'],\n",
    "            num_res_units=2,\n",
    "            norm = norm_type,\n",
    "            dropout = dropout)\n",
    "\n",
    "model.load_state_dict(torch.load(model_state, map_location = device))\n",
    "model = model.to(device)\n",
    "\n",
    "inferer = SlidingWindowInferer(roi_size=patch_size, sw_batch_size=batch_size,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c6d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec74e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from list\n"
     ]
    }
   ],
   "source": [
    "#pick test image\n",
    "raw_path = filedialog.askopenfilename()\n",
    "raw_img = glob.glob(raw_path)\n",
    "\n",
    "# mask_path = filedialog.askopenfilename()\n",
    "# mask_img = glob.glob(mask_path)\n",
    "\n",
    "# raw_img = ['E:\\\\Image_Folder\\\\Raw\\\\000_B_181107_A_N1B2_4a61736f.tif']\n",
    "# mask_img = ['E:\\\\Image_Folder\\\\Mask\\\\000_B_181107_A_N1B2_4a61736f.tif']\n",
    "\n",
    "segmentation_exp = experiment\n",
    "ex_autofluor = False # True/False\n",
    "ex_melanocytes = True # True/False\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "patch_transform = transforms.Compose([MinMaxScalerVectorized()])\n",
    "label_transform = transforms.Compose([process_masks(exp = segmentation_exp,\n",
    "                                                    ex_autofluor=ex_autofluor,\n",
    "                                                    ex_melanocytes=ex_melanocytes,\n",
    "                                                     )])\n",
    "\n",
    "processed_set = WholeVolumeDataset(raw_directory = raw_img,\n",
    "                                   num_classes = output_chnl,\n",
    "                                   raw_transform = patch_transform,\n",
    "                                   label_transform = label_transform,\n",
    "                                   mask_order = dim_order,\n",
    "                                   device = device,\n",
    "                                   )\n",
    "\n",
    "# processed_dataloader = DataLoader(processed_set, batch_size=1, shuffle= False)\n",
    "\n",
    "raw, mask = next(iter(processed_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f95570b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = torch.unsqueeze(raw, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36b269db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [04:23<00:00, 14.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# predict using shifted windows\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = inferer(inputs = raw, network=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08380fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_from_categorical = to_numpy(torch.argmax(pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329957e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca58d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred_from_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bf2dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(\"SLAP_INFERENCE_TEST_16x128x128.tif\", pred_from_categorical.astype(np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(pred_from_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5953174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "# viewer = napari.Viewer()\n",
    "# orig_img = tifffile.imread(raw_img)\n",
    "# raw_image = viewer.add_image(orig_img, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577942a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_img = viewer.add_labels(reconstructed_img.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e794aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2ca3851",
   "metadata": {},
   "source": [
    "## 2D Inferencing using SliceInferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"+s_+d_-f\"\n",
    "model_soma_dendrite = \"+s_+d_-f_ResUNet_1_77.pth\"\n",
    "model_path = f\"C:\\\\Users\\\\Fungj\\\\Google Drive\\Masters Project\\\\Segmentation-Model\\\\3D-Neuron-Segment\\\\results\\\\ResUNET\\\\20230111\\\\{experiment}\\\\{model_soma_dendrite}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lateral_steps = 512\n",
    "patch_size = (lateral_steps, lateral_steps)\n",
    "batch_size = 1\n",
    "input_chnl = 1\n",
    "output_chnl = 4\n",
    "norm_type = \"batch\"\n",
    "dropout = 0.1\n",
    "\n",
    "model = UNet(spatial_dims=2, \n",
    "            in_channels = input_chnl,\n",
    "            out_channels = output_chnl,\n",
    "            channels = (32, 64, 128, 256, 512),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            norm = norm_type,\n",
    "            dropout = dropout)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "# inferer = SlidingWindowInferer(roi_size=patch_size, sw_batch_size=batch_size)\n",
    "inferer = SliceInferer(roi_size=patch_size, sw_batch_size=batch_size, spatial_dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick test image\n",
    "# raw_path = filedialog.askopenfilename()\n",
    "# raw_img = glob.glob(raw_path)\n",
    "\n",
    "# mask_path = filedialog.askopenfilename()\n",
    "# mask_img = glob.glob(mask_path)\n",
    "raw_img = ['E:\\\\Image_Folder\\\\Raw\\\\000_ML_20180613_N4_4a61736f.tif']\n",
    "mask_img = ['E:\\\\Image_Folder\\\\Mask\\\\000_ML_20180613_N4_4a61736f.tif']\n",
    "\n",
    "segmentation_exp = experiment\n",
    "ex_autofluor = False # True/False\n",
    "ex_melanocytes = True # True/False\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "patch_transform = transforms.Compose([MinMaxScalerVectorized()])\n",
    "label_transform = transforms.Compose([process_masks(exp = segmentation_exp,\n",
    "                                                    ex_autofluor=ex_autofluor,\n",
    "                                                    ex_melanocytes=ex_melanocytes,\n",
    "                                                     )])\n",
    "\n",
    "processed_set = WholeVolumeDataset(raw_directory = raw_img,\n",
    "                                   mask_directory = mask_img,\n",
    "                                   num_classes = output_chnl,\n",
    "                                   raw_transform = patch_transform,\n",
    "                                   label_transform = label_transform,\n",
    "                                   mask_order = dim_order,\n",
    "                                   device = device,\n",
    "                                   )\n",
    "\n",
    "# processed_dataloader = DataLoader(processed_set, batch_size=1, shuffle= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9452b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, mask = next(iter(processed_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = inferer(inputs = raw, network=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ef735",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.softmax(pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3907dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_from_categorical = to_numpy(torch.argmax(probabilities, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b718d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_from_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb540b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred_from_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474da0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(\"000_ML_20180613_N4_4a61736f_INFERENCED_fold_1.tif\", pred_from_categorical.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2b060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6ca2fc8813f9b6ddd55f1976887f836d1e81bcaf3963c7a3c14df03143688c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
